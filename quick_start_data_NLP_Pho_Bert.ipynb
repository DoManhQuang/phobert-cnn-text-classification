{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "quick_start_data_NLP",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3ct-sMHxig4"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/duyvuleo/VNTC.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unrar x -Y \"/content/VNTC/Data/10Topics/Ver1.1/Test_Full.rar\" \"/content/VNTC/Data/10Topics/Ver1.1/\"\n",
        "!unrar x -Y \"/content/VNTC/Data/10Topics/Ver1.1/Train_Full.rar\" \"/content/VNTC/Data/10Topics/Ver1.1/\""
      ],
      "metadata": {
        "id": "LOEauCVCyAk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install fairseq\n",
        "!pip3 install fastbpe\n",
        "!pip3 install vncorenlp\n",
        "!pip3 install transformers\n",
        "!pip3 install underthesea"
      ],
      "metadata": {
        "id": "KmD53liwyzr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import torch\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import numpy as np\n",
        "import pickle\n",
        "from vncorenlp import VnCoreNLP\n",
        "import underthesea"
      ],
      "metadata": {
        "id": "t0iEvhNpywNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hODNTdhvsoG8",
        "outputId": "3a397e1f-cee1-40ff-c0e1-2c1398b0ed6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_dump(file_path, data, labels):\n",
        "    file = open(file_path, 'wb')\n",
        "    # dump information to that file\n",
        "    pickle.dump((data, labels), file)\n",
        "    # close the file\n",
        "    file.close()\n",
        "    pass\n",
        "\n",
        "\n",
        "def load_data(path_file):\n",
        "    file = open(path_file, 'rb')\n",
        "    # dump information to that file\n",
        "    (pixels, labels) = pickle.load(file)\n",
        "    # close the file\n",
        "    file.close()\n",
        "    print(pixels.shape)\n",
        "    print(labels.shape)\n",
        "    return pixels, labels\n",
        "\n",
        "\n",
        "def load_pho_bert():\n",
        "    pho_bert = AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
        "    return pho_bert, tokenizer\n",
        "\n",
        "\n",
        "def standardize_data(row):\n",
        "    # Xóa dấu chấm, phẩy, hỏi ở cuối câu\n",
        "    row = re.sub(r\"[\\,\\?]+$-()!*=._\", \"\", row)\n",
        "    row = row.replace(\",\", \" \") \\\n",
        "        .replace(\";\", \" \").replace(\"“\", \" \") \\\n",
        "        .replace(\":\", \" \").replace(\"”\", \" \") \\\n",
        "        .replace('\"', \" \").replace(\"'\", \" \") \\\n",
        "        .replace(\"!\", \" \").replace(\"?\", \" \") \\\n",
        "        .replace(\"-\", \" \").replace(\"*\", \" \")\\\n",
        "        .replace(\"=\", \" \").replace(\"(\", \" \")\\\n",
        "        .replace(\")\", \" \").replace(\"_\", \" \").replace(\".\", \" \")\n",
        "    row = row.strip().lower()\n",
        "    return row\n",
        "\n",
        "\n",
        "def get_max_decode_token(v_token, max_len):\n",
        "    data_token = []\n",
        "    cnt_code = 0\n",
        "    for code in v_token:\n",
        "        if cnt_code >= max_len:\n",
        "            break\n",
        "        data_token.append(code)\n",
        "        cnt_code += 1\n",
        "    # print(len(data_token))\n",
        "    return data_token\n",
        "\n",
        "\n",
        "def get_text_feature(sentence, v_pho_bert, v_tokenizer):\n",
        "    v_tokenized = []\n",
        "    max_len_word = 100\n",
        "    word_segmented_text = underthesea.word_tokenize(sentence)\n",
        "    \n",
        "    line = \" \".join(word_segmented_text)\n",
        "    line = underthesea.word_tokenize(line, format=\"text\")\n",
        "\n",
        "    v_token = v_tokenizer.encode(line)\n",
        "    # print(v_token)\n",
        "\n",
        "    data_token = get_max_decode_token(v_token, max_len_word)\n",
        "    v_tokenized.append(data_token)\n",
        "\n",
        "    padded = np.array([i + [1] * (max_len_word - len(i)) for i in v_tokenized])\n",
        "    # print(padded)\n",
        "    # print(padded.shape)\n",
        "\n",
        "    # Đánh dấu các từ thêm vào = 0 để không tính vào quá trình lấy features\n",
        "    attention_mask = np.where(padded == 1, 0, 1)\n",
        "    # print('attention mask:', attention_mask[0])\n",
        "\n",
        "    # Chuyển thành tensor\n",
        "    padded = torch.tensor(padded).to(torch.long)\n",
        "    # print(padded)\n",
        "    # print(\"Padd = \", padded.size())\n",
        "    attention_mask = torch.tensor(attention_mask)\n",
        "    # print(attention_mask)\n",
        "\n",
        "    # Lấy features dầu ra từ BERT\n",
        "    with torch.no_grad():\n",
        "        last_hidden_states = v_pho_bert(input_ids=padded, attention_mask=attention_mask)\n",
        "\n",
        "    v_features = last_hidden_states[0].numpy().T\n",
        "    print(v_features.shape)\n",
        "\n",
        "    return v_features\n",
        "\n",
        "\n",
        "def load_data_post_directory(DIRECTORY, CATEGORIES, cnt_max_category):\n",
        "    print(\"[INFO] loading post...\")\n",
        "    data = []\n",
        "    labels = []\n",
        "    pho_bert, v_token = load_pho_bert()\n",
        "    for category in CATEGORIES:\n",
        "        path = os.path.join(DIRECTORY, category)\n",
        "        print(path)\n",
        "        cnt_category = 0\n",
        "        for post in os.listdir(path):\n",
        "            if cnt_category == cnt_max_category:\n",
        "                break\n",
        "            post_path = os.path.join(path, post)\n",
        "            print(post_path)\n",
        "            f = open(post_path, \"r\", encoding='utf-16')\n",
        "            text_post = f.read()\n",
        "            text_post = standardize_data(text_post)\n",
        "\n",
        "            v_feat = get_text_feature(text_post, pho_bert, v_token)\n",
        "\n",
        "            data.append(v_feat)\n",
        "            labels.append(category)\n",
        "            cnt_category += 1\n",
        "\n",
        "    dataset = np.array(data, dtype=\"float32\")\n",
        "    labels = np.array(labels)\n",
        "    return dataset, labels"
      ],
      "metadata": {
        "id": "Lwt49EoyziGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset\n",
        "DIRECTORY_test = \"/content/VNTC/Data/10Topics/Ver1.1/Test_Full\"\n",
        "DIRECTORY_train = \"/content/VNTC/Data/10Topics/Ver1.1/Train_Full\"\n",
        "CATEGORIES = ['Chinh tri Xa hoi', 'Doi song', 'Khoa hoc', 'Kinh doanh', 'Phap luat', 'Suc khoe',\n",
        "              'The gioi', 'The thao', 'Van hoa', 'Vi tinh']"
      ],
      "metadata": {
        "id": "mZAaVEIuqjDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train, labels_train = load_data_post_directory(DIRECTORY_train, CATEGORIES, 700)"
      ],
      "metadata": {
        "id": "3Auk3_9iukZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_test, labels_test = load_data_post_directory(DIRECTORY_test, CATEGORIES, 300)"
      ],
      "metadata": {
        "id": "VAZ7FaD7KdyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_dump('/content/drive/MyDrive/data_train.data', data_train, labels_train)\n",
        "save_dump('/content/drive/MyDrive/data_test.data', data_test, labels_test)\n",
        "\n",
        "print(data_train.shape)\n",
        "print(data_test.shape)\n",
        "\n",
        "print(labels_train.shape)\n",
        "print(labels_test.shape)"
      ],
      "metadata": {
        "id": "KsUkEVP4KhFj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7c6a76b-762f-4d60-80c9-f1df3aa6c116"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1400, 768, 100, 1)\n",
            "(1400,)\n"
          ]
        }
      ]
    }
  ]
}